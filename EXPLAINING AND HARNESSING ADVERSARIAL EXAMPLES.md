# EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES 论文笔记

[原论文链接](https://arxiv.org/pdf/1412.6572.pdf)

## 摘要

包括神经网络在内的许多机器学习模型都很容易受到对抗样本（adversarial examples）的攻击导致误分类，对抗性样本指通过刻意的给输入增加一些轻微的恶意扰动，从而使系统以高置信度对样本进行错误的分类。之前的一些观点认为这种现象是由于非线性性质和过拟合导致的，但是本文认为神经网络对于对抗性干扰的易损性（vulnerability）的本质原因是他们在高维空间的线性性质。而最有趣的现象在于他们具有跨架构和训练集的泛化能力，本文是最先对这种现象给出解释的论文，并且这个解释得到了新的定量结果的支持。此外，本文还提供了一种简单而快速的方法来生成对抗样本，使用这种方法来提供对抗性训练的样本，可以减少 MNIST 数据集上的测试误差。

## 简介

对抗样本易损性：通过刻意的给输入增加一些轻微的恶意扰动，就可以使机器学习模型以高置信度对样本进行错误的分类。**在很多情况下，同一个对抗样本可以让使用不同架构，在不同训练集下训练的不同模型做出误判**，这说明对抗样本暴露了训练算法中的一些共通的基础盲点。

这些现象的原因是一个谜，一种推测认为这是由于深度神经网络的极端非线性，可能使模型均值化不充分以及纯监督学习的正规化不充分相结合。但本文证明这种猜测是不成立的，**高维空间的线性性质足以导致对抗性样本产生**，这种观点使得我们可以设计一种快速的方法来产生对抗样本以及进行对抗性训练。对抗性样本可以产生比 dropout 更优秀的正则化收益，通用的正则化策略并不能显著的降低模型的对抗样本易损性，而将模型转变为非线性模型族就有可能做到，例如 RBF 网络。

本文展示了一种权衡，是将模型训练为容易训练的线性模型，还是训练为复杂的非线性模型以抵御对抗样本的干扰。长远来看，设计更多训练非线性模型的更强大的优化方法可以避免这种问题。

## 相关工作

Szegedy 等人在论文 [Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199.pdf) 中展示了神经网络的一些有趣的性质，本文涉及的有：

- **Box-constrained L-BFGS** 可以有效的找出对抗性样本
- 在一些数据集中，对抗性样本和原样本非常相似，人眼难以找出差别
- 在很多情况下，同一个对抗样本可以让使用不同架构，在不同训练集下训练的不同模型做出误判
- 浅层的 softmax 回归模型也具有对抗样本易损性
- 使用对抗性样本进行训练可以使模型正则化，但是这种方法需要在内循环中进行约束性优化，时间成本非常高昂，在实践中难以实现。

这些性质就表明即使是在测试集上表现优异的分类模型，实际上也并没有学习到真正决定正确类别的内在概念。恰恰相反，这些算法就像一个 [Potemkin village](https://en.wikipedia.org/wiki/Potemkin_village)，在自然发生的数据中表现良好，但是当出现概率非常低的数据的时候，就会产生虚假的输出。这点尤其让人感到失望，因为计算机视觉的主流方法就是使用CNN的特征空间上用欧氏距离近似感知距离，但是如果感知距离非常小的图像对应于网络表征中的完全不同的类，那么这种相似性显然是有缺陷的。但是，这种缺陷在某种程度上也正好是用来修复这个问题的方式。

## 对抗样本的线性解释

因为样本输入特征（input feature）的精度（precision）是有限的，比如一般图像的每个像素是8 bits, 那么样本中所有低于$1/255$ 的信息都会被丢弃，所以**当样本 $x$ 中每个元素值添加的扰动值 $\eta$ 小于样本输入特征精度时，分类器无法将样本 $x$ 和对抗样本 $$\tilde{\boldsymbol{x}}=\boldsymbol{x}+\boldsymbol{\eta}$$ 区分开。**因此，对一个区分良好的类别而言，如果 $\epsilon$ 是一个足够小以至于被舍弃掉的值，那么只要 $\|\eta\|_{\infty}<\epsilon$，分类器就会将 $x$ 和 $\tilde{x}$ 分为同一个类。

思考一下权重向量 $w$ 和对抗样本 $\tilde{x}$ 的点积（dot product）：
$$
w^{\top} \tilde{x}=w^{\top}(x+\eta)=w^{\top} x+w^{\top} \eta
$$
对抗性干扰导致 activation 增加了 $w^{\top} \eta$，本文指出可以使用最大范数约束（maxnorm constraint）将 $\eta$ 赋值为 $\eta=\operatorname{sign}(\boldsymbol{w})$ 从而使 $\boldsymbol{w}^{\top} \boldsymbol{\eta}$ 最大化。如果 $w$ 有 $n$ 维且元素均值为 $m$，那么 activation 将增加 $\epsilon mn$，虽然 $\|\eta\|_{\infty}$ 不会随着维度改变，但是由 $\eta$ 导致的 activation 的增加量 $\epsilon mn$ 会随着维度 $n$ 线性增长。**那么对于一个高维度的问题，一个样本中大量维度的无限小的干扰加在一起就可以对输出造成很大的变化。**这说明如果具有足够高的维度，即使是一个简单的线性模型也具有对抗样本易损性。

## 非线性模型的线性扰动

我们认为神经网络过于线性以至于不能抵抗对抗样本的干扰，而这种观点引出了一种生成对抗样本的快速方法，即**Fast Gradient Sign Method (FGSM)**方法。

![adversarial examples](D:\学习笔记\figure\adversarial examples.jpg)

让 $\theta$ 作为模型的参数，$x$ 作为模型的输入，$y$ 是模型对应的 label 值，$J(\theta,x,y)$ 是训练神经网络的损失函数，对某个特定的模型参数 $\theta$ 而言，**FGSM**方法将损失函数近似线性化，从而获得保证无穷范数限制的最优的扰动 $\|\eta\|_{\infty}<\epsilon$，可以使用反向传播有效地计算所需的梯度。扰动值具体为：
$$
\boldsymbol{\eta}=\epsilon \operatorname{sign}\left(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \boldsymbol{x}, y)\right)
$$
实验表明，**FGSM** 这种简单的算法确实可以产生误分类的对抗样本，从而证明了作者假设的对抗样本的产生原因是由于模型的线性特性。同时，这种算法也可作为一种加速对抗训练的方法。

## 线性模型与权重衰减的对抗训练

通过对最简单的逻辑回归（logistic regression）模型上应用 FGSM 方法，理解如何在简单的设置中生成对抗样本。

![FGSM LR](D:\学习笔记\figure\FGSM LR.PNG)

如果我们训练了一个单一模型来对 $P(y=1)=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)$ 区分标签 $y \in\{-1,1\}$，其中 $\sigma(z)$ 是 logistic sigmoid 函数 $\sigma(z)=\frac{1}{1+e^{-z}}$，使用梯度下降训练的损失函数为：
$$
\mathbb{E}_{\boldsymbol{x}, y \sim p_{\text { data }}} \zeta\left(-y\left(\boldsymbol{w}^{\top} \boldsymbol{x}+b\right)\right)
$$
其中 $\zeta(z)$ 为 softplus 函数 $\zeta(z)=log(1+e^z)$。

对该模型使用 **FGSM** 方法，扰动量 $\eta$ 为：
$$
\begin{array}{l}{\eta=\epsilon \operatorname{sign}\left(\nabla_{x} J(\theta, x, y)\right)\\{=\epsilon \operatorname{sign}\left(\nabla_{x} \zeta\left(-y\left(w^{\top} x+b\right)\right)\right)}} \\ {=\epsilon \operatorname{sign}\left(-w^{\top} \star \sigma\left(-\left(w^{\top} x+b\right)\right)\right)\\{=\epsilon \operatorname{sign}(-w)}\\{=-\epsilon \operatorname{sign}(w)}}\end{array}
$$
并且：
$$
\boldsymbol{w}^{\top} \operatorname{sign}(\boldsymbol{w})=\|\boldsymbol{w}\|_{1}
$$
因此逻辑回归模型的对抗形式为最小化下列损失函数：
$$
E_{x, y \sim p_{\text {data}}} \zeta\left(-y\left(w^{\top} \tilde{x}+b\right)\right) \underbrace{=}_{\tilde{x}=x+\eta, \eta=\epsilon-\operatorname{sign}(w)} E_{x, y \sim p_{\text { data }}} \zeta\left(y\left(\epsilon| | w| |_{1}-w^{\top} x-b\right)\right)
$$
这就跟 $L^1$ 正则化类似，但不同的是对抗形式是在训练过程中减去 $L^1$ 惩罚项，而不是加上 $L^1$ 惩罚项。也就是说如果模型学会了以足够高的置信度做出 $\zeta$ 饱和的预测，那么惩罚项最终会消失。 这不一     定会发生，因为在欠拟合的场景下，对抗性训练只会加剧欠拟合。 

## 深度网络的对抗训练

**人们通常误以为深度学习的方法更容易受到对抗攻击，但是实际上跟浅层线性模型相比，深度网络至少可以在训练网络过程中抵御对抗扰动攻击。**因为只有深度学习有能力去拟合一个非线性模型，而这是浅层模型所无法处理的。The universal approximator theorem 表明，只要隐藏层拥有足够多的的神经元，具有至少一个隐藏层的神经网络可以表示任意精度的任何函数，因此深度学习能够学习到一个函数能够抵御对抗攻击而其他浅层模型不行。浅层模型不能做到说在不同输入给出不同输出的同时还要给临近的输入得到相同的输出。当然了，也没有理论证明算法是否能发现一个能够完全符合我们期望的函数，就像标准的有监督训练并不能保证能够学习到能够抵御对抗样本的函数，这个过程需要显式的在训练过程体现。

对抗样本的训练与其他数据扩充方案有些不同，通常，人们通过转换来扩充数据，例如预期在测试集中实际发生的转换，而对抗样本的扩充需要转换为自然情况下不太可能出现的输入，从而暴露出模型的缺陷。这种情况下，dropout 并不能在前沿基准中做出提升，这在一定程度上也是因为基于 **L-BFGS** 的对抗性例子成本太高，很难广泛地进行实验。

本文提出了一种利用 **FGSM** 进行对抗训练的方法，目标函数为：
$$
\tilde{J}(\theta,x,y)=\alpha J(\theta,x,y)+(1-\alpha)J(\theta,x+\epsilon sign(\nabla{x}J(\theta,x,y))
$$
本文的实验中用的 $\alpha$ 值为 0.5，这种对抗训练的方法意味着在训练过程中不断更新对抗样本，从而使得当前模型可以抵御对抗样本。

但是作者表示在训练集上对抗训练的错误率error rate没有达到过0%，作者主要从以下两个方面解决：

- 增大模型，即使用1600个unit代替240个unit
- 在对抗训练中，验证集错误率会随着时间逐渐平稳，但是虽然验证集错误率比较小，但是对抗验证错误率并不小，因此在对抗验证错误率 adversarial validation set error 上也使用early stopping算法

文章表明，在不进行对抗训练的情况下，模型识别 **FGSM** 攻击方法生成样本的错误率是89.4%，但是通过对抗训练，同样的模型识别对抗样本的错误率下降到17.9%。然而，当模型对对抗样本误分类的时候，置信度依然很高，平均置信度为81.4%。本文还发现，学习模型的权重发生了显著的变化，而对抗训练模型的权重具有显著的本地化和可解释性。

对抗性训练过程可以被看作是在数据受到对抗干扰时最小化 worst-case 错误率，这可以被解释为进行了一场对抗游戏，或者是最小化把 $U(-\epsilon,\epsilon)$ 加入输入时的噪声样本上的损失上界。对抗训练也可以被认为是一种主动学习 （active learning） 的形式，这种情况下，模型可以在新的点上请求标签，人工标签机 （human labeler）被一个启发式的标签机（heuristic labeler）所取代，该标签机从附近的点复制标签。

## **对抗样本泛化原因**

很多论文都表明对抗样本具有Transferability。具体来说，在一个特定模型上产生的对抗样本通常也容易被其他模型误分类，即使这些模型的结构不同或者模型在不同的训练集上训练。甚至，不同的模型对对抗样本误分类的结果相同！这种泛化特征意味着如果有人希望对模型进行恶意攻击，攻击者根本不必访问需要攻击的目标模型，就可以通过训练自己的模型来产生对抗样本，然后将这些对抗样本部署到他们需要攻击的模型中。作者表明，非线性或者过拟合的假设不能解释为何拥有无限能力的极度非线性模型会以相同的方式标注数据分布点。

在本文提出的线性解释下，作者认为对抗样本在广泛的子空间存在。

![argument_softmax](D:\学习笔记\figure\argument_softmax.jpg)

上图表明，在不同的 $\epsilon$ 下，可以看到FGSM可以在一维的**连续子空间**内产生对抗样本，**而不是特定的区域**。这就解释了为什么对抗样本特别多，以及为什么对抗样本transferability存在。

另外，为了解释为什么不同的分类器将对抗样本误分类到同一个类，作者假设目前的方法训练神经网络都类似于在同一个训练集上学习的线性分类器。由于机器学习算法的泛化能力，所以线性分类器可以在训练集的不同子集上训练出大致相同的分类权重。底层分类权重的稳定性反过来又会导致对抗样本中的稳定性。

## **对抗样本存在性的其他假设**

作者通过实验及分析，反驳了其他两种对抗样本存在性的假设。

**假设1：**生成训练可以在训练过程中提供更多的限制，或者是的模型学习如何分辨"real"或者"fake"的数据，并且对"real"的数据置信度更高。

文章表明，某些生成训练并不能达到假设的效果，但是不否认可能有其他形式的生成模型可以抵御攻击，但是确定的是生成训练的本身并不足够。

**假设2：**对抗样本存在于单个奇怪的模型(models with strange quirks)，因此多个模型的平均可以使得模型防御性更好。

文章通过实验说明，模型融合对于对抗样本的防御能力非常有限。